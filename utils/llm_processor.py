"""
Module pour l'extraction de donn√©es structur√©es avec Ollama LLM
Utilise l'API structur√©e d'Ollama avec Pydantic pour une validation robuste
"""

from typing import List, Optional
from pydantic import BaseModel, Field, ValidationError
from ollama import chat


class ProductExtraction(BaseModel):
    """
    Sch√©ma Pydantic pour un produit extrait
    """
    product_identifier: str = Field(
        ..., 
        description="SKU, code produit ou identifiant unique",
        min_length=1,
        max_length=255
    )
    name: str = Field(
        ..., 
        description="Nom du produit",
        min_length=1,
        max_length=500
    )
    price: float = Field(
        ..., 
        description="Prix du produit (nombre d√©cimal positif)",
        gt=0.0
    )
    currency: str = Field(
        default="DT", 
        description="Code devise ISO (DT,TND, EUR,USD,$, etc.)",
        max_length=3
    )
    category: Optional[str] = Field(
        None, 
        description="Cat√©gorie du produit",
        max_length=255
    )
    description: Optional[str] = Field(
        None, 
        description="Description d√©taill√©e du produit"
    )
    product_url: Optional[str] = Field(
        None, 
        description="URL compl√®te de la page produit"
    )
    image_url: Optional[str] = Field(
        None, 
        description="URL de l'image principale du produit"
    )
    is_available: bool = Field(
        default=True, 
        description="Disponibilit√© du produit (en stock ou non)"
    )
  
class LLMResponse(BaseModel):
    """
    Sch√©ma Pydantic pour la r√©ponse compl√®te du LLM
    """
    products: List[ProductExtraction] = Field(
        default_factory=list,
        description="Liste des produits extraits"
    )
    promotions: List[str] = Field(
        default_factory=list,
        description="Liste des promotions d√©tect√©es"
    )

def extract_products_with_llm(text_batch: str, competitor_base_url: str, model: str = 'llama3.1') -> LLMResponse:
    """
    Extrait les produits d'un texte en utilisant Ollama avec sortie structur√©e
    
    Args:
        text_batch: Texte nettoy√© √† analyser
        competitor_base_url: URL du site concurrent (pour contexte)
        model: Nom du mod√®le Ollama √† utiliser (d√©faut: llama3.1)
    
    Returns:
        LLMResponse: Objet Pydantic contenant les produits et promotions extraits
    """
    
    # Prompt optimis√© pour l'extraction structur√©e
    system_prompt = """Tu es un expert en extraction de donn√©es pour l'e-commerce.
Ta mission est d'analyser du texte provenant de sites web concurrents et d'extraire TOUTES les informations produits de mani√®re structur√©e et pr√©cise.

R√àGLES STRICTES:
1. Extrais uniquement les informations pr√©sentes dans le texte
2. Ne g√©n√®re JAMAIS de donn√©es fictives ou invent√©es
3. Si un champ est incertain, utilise null
4. Pour les prix, extrais uniquement la valeur num√©rique (retire symboles et espaces)
5. Identifie le code produit (SKU, r√©f√©rence, mod√®le) comme product_identifier
6. D√©tecte les promotions et offres sp√©ciales s√©par√©ment

FORMAT DE SORTIE:
Le JSON doit suivre exactement ce sch√©ma avec ces champs obligatoires:
- products: liste d'objets avec (product_identifier, name, price, currency, category?, description?, product_url?, image_url?, is_available)
- promotions: liste de textes d√©crivant les offres promotionnelles d√©tect√©es"""

    user_prompt = f"""Analyse le texte suivant provenant du site: {competitor_base_url}

TEXTE √Ä ANALYSER:
{text_batch[:5000]} 

Extrait TOUS les produits avec leurs informations compl√®tes."""

    try:
        # Appel √† l'API Ollama avec sch√©ma structur√© (nouvelle API)
        response = chat(
            model=model,
            messages=[
                {
                    'role': 'system',
                    'content': system_prompt
                },
                {
                    'role': 'user',
                    'content': user_prompt
                }
            ],
            format=LLMResponse.model_json_schema(),  # Force le LLM √† suivre le sch√©ma Pydantic
            options={
                'temperature': 0.1,      # Tr√®s d√©terministe pour extraction de donn√©es
                'top_p': 0.9,
                'num_predict': 800,     # Limite de tokens g√©n√©r√©s
            }
        )
        
        # Extraction du contenu de la r√©ponse
        raw_content = response['message']['content']
        print(f"R√©ponse LLM brute re√ßue: {raw_content[:2500]}")
        
        # Validation avec Pydantic
        try:
            validated_response = LLMResponse.model_validate_json(raw_content)
            print(f"‚úÖ Extraction r√©ussie: {len(validated_response.products)} produits trouv√©s")
            print(f"produits trouv√©s: {validated_response}")
            return validated_response
            
        except ValidationError as e:
            print(f"‚ùå Erreur de validation Pydantic: {e}")
            print(f"Contenu brut qui a √©chou√©: {raw_content[:2500]}")
            # Retourner une r√©ponse vide plut√¥t que de crasher
            return LLMResponse(products=[], promotions=[])

    except Exception as e:
        print(f"‚ùå Erreur lors de l'appel Ollama: {type(e).__name__}: {e}")
        return LLMResponse(products=[], promotions=[])


def extract_products_with_retry(
    text_batch: str, 
    competitor_base_url: str, 
    max_retries: int = 2,
    model: str = 'llama3.1'
) -> LLMResponse:
    """
    Wrapper avec logique de retry pour g√©rer les √©checs temporaires
    
    Args:
        text_batch: Texte √† analyser
        competitor_base_url: URL du concurrent
        max_retries: Nombre maximum de tentatives
        model: Mod√®le Ollama √† utiliser
    
    Returns:
        LLMResponse: R√©sultat de l'extraction
    """
    for attempt in range(max_retries):
        print(f"üîÑ Tentative {attempt + 1}/{max_retries} d'extraction LLM")
        
        result = extract_products_with_llm(text_batch, competitor_base_url, model)
        
        # Si au moins un produit trouv√©, c'est un succ√®s
        if result.products:
            print(f"‚úÖ Succ√®s √† la tentative {attempt + 1}")
            return result
        
        # Si derni√®re tentative et toujours rien, retourner r√©sultat vide
        if attempt == max_retries - 1:
            print(f"‚ö†Ô∏è Aucun produit extrait apr√®s {max_retries} tentatives")
            return result
        
        print(f"‚ö†Ô∏è Tentative {attempt + 1} n'a trouv√© aucun produit, retry...")
    
    return LLMResponse(products=[], promotions=[])
